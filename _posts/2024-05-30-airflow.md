---
layout: article
title: Airflow로 batch process 자동화하기
aside:
  toc: true
cover: /assets/backend_mlops/airflow_base.png
excerpt: Airflow를 이용하여 batch 프로세스를 자동화하는 방법에 대해 알아보겠습니다.
---

데이터를 분석하다보면 생각보다 단순 반복 업무가 굉장히 많다. 

늘 비슷한 포맷의 데이터가 들어오고 나는 그걸 똑같은 코드에 넣고 실행해서 결과를 보고 이상이 없으면 패스한다. 

이것도 1년을 넘게 하다보니 이제는 좀 지겹다. 그래서 이것들을 자동화할 수 있는 파이프라인을 구축해보려고 한다. 

<br>

우리 회사는 간헐적으로 고객사가 데이터를 NAS에 업로드해주면 이를 한번에 다운로드 받아 처리하고 결과를 전달해준다. 

이런 배치 프로세스에 적합한 전처리 툴이 뭐가 있을지 고민하다보니 Airflow를 많이 추천해주길래 한번 사용해봤다. 

<br>

>Airflow는 파이썬 코드를 기반으로 워크플로우를 작성, 예약 및 모니터링 할 수 있게 하는 오픈소스 플랫폼이다.   
>파이썬을 이용해 GUI 기반의 데이터 파이프라인을 만들 수 있고, 정해진 시간에 이를 자동으로 실행할 수 있게 해준다.
 

출처 - [Passwd 블로그](https://passwd.tistory.com/entry/Apache-Airflow%EB%9E%80)

<br>

저 GUI를 이용해 클릭 한번으로 전체 파이프라인을 실행 시킬 수 있다는 점에 혹해서 바로 도입을 해봤다. 

다른 개발자들은 터미널에서 개발하는게 멋지다고 하지만 나는 이상하게 클릭 한 번에 원하는 바가 딱딱 이루어지는 심플함이 좋다. 

<br>

아무튼, 이번 포스팅에서는 Airflow를 설치하는 과정과 이를 이용해 파이프라인을 구축하는 과정에 대해 설명하고자 한다. 

파이프라인을 webserver에 업로드하고 실행하는 과정은 다음 포스팅을 참고해주길 바란다. 

<br>

<br>

# 1. Airflow 사용 usecase

<br>

이해를 돕기 위해 먼저 Airflow가 분석에서 어떻게 사용되는지 확인 먼저 해보고 가겠다. 

당신이 아래와 같은 상황에 놓여 있다고 가정하자. 

<br>

-   밤사이에 데이터가 S3에 쌓임 
-   매일 아침 9시에 해당 데이터들을 일괄적으로 다운로드 받아 분석 시작(배치 프로세스) 
-   preprocess.py에 구현된 전처리 함수를 순서대로 동작시키며 데이터를 원하는 형태로 전처리 후 mongoDB에 저장 
-   이를 매일 반복 

<br>

일단은 최대한 간단한 시나리오를 가정해봤다. 

너무 어려우면 처음 airflow를 보시는 분들은 저게 뭐에 사용되는 놈인지 파악하기 어려울것이고 나도 설명하기 힘들다. 

<br>

위의 시나리오는 간단하지만 매일 반복해야하는 업무이다. 

S3에 접속해서 데이터 다운로드를 누르고 `preprocess.py` 코드를 찾아 실행한 후 mongoDB에 저장하는 과정을 매일매일 반복해야한다. 

<br>

Airflow는 이러한 전처리를 자동으로 할 수 있는 강력한 도구이다. 

앞서 작성해둔 preprocess.py를 조금만 수정하면 간단한 파이프라인 그래프를 그릴 수 있고, 이를 Airflow webserver로 보내 GUI로 관리할 수 있다. 

우리는 GUI에서 파이프라인 우측 상단의 재생 버튼만 누르면 된다.

(하단 그림 참고) 그러면 저 귀찮은 과정을 모두 순서대로 착착 진행해준다. 

<br>

![airflow_base](/assets/backend_mlops/airflow_base.png)

<br>

매일 아침 재생 버튼을 누르는 것도 귀찮다면 아침 9시에 자동으로 파이프라인이 실행되도록 스케줄링 해줄 수도 있다. 

그리고 분석이 끝나면 이를 슬랙을 통해 데이터 사이언티스트에게 알림도 보낼 수 있다. 

이 얼마나 편리한 도구인가? 매일매일 귀찮은 단순 노동에서 벗어나서 분석에만 집중할 수 있는 환경을 만들어준다. 

<br>

이를 보고 혹해서 '나도 분석 환경에 airflow 한번 적용해볼까?'하는 분들이 있으시다면 다음의 과정들이 궁금할 것이다. 

<br>

-   어떻게 airflow를 설치해? 
-   .py 파일을 수정하면 파이프라인 그래프를 그릴 수 있다는데 어떻게 수정해야해? 많이 수정해야돼? 
-   파이썬 파일 말고 다른 언어로도 파이프라인을 만들 수 있어? 
-   만들어진 파이프라인은 GUI환경으로 어떻게 업로드해?(다음 포스팅에서 다룰 예정)
-   파이프라인 스케줄링(자동 실행)은 따로 설정해줘야하는게 있어?(다음 포스팅에서 다룰 예정)

<br>

아래에서 위 질문들에 대해 하나하나 답을 해보겠다. 

<br>

<br>

# 2. Airflow 설치 

<br>

Airflow를 설치하는 과정은 공식 문서에도 잘 나와 있다. 

여러 방법이 있는데 PyPI를 이용하면 간단하게 설치 가능하다고 한다. 

<br>

[Airflow 공식 문서 설치 가이드](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html)

<br>

그런데, 나는 저 방식으로 설치가 되지 않았다. 

설치 과정은 문제없이 진행되지만 잘 설치되었는지 확인하기 위해 airflow version을 입력하면 그런거 설치 안되있다고 한다.

<br>

여러 리소스를 뒤지다보니 아래 블로그대로 실행을 하니 잘 설치되는 것을 확인하였다. 

혹시 나와 비슷한 문제를 겪는 사람들이 있다면 아래 블로그를 보고 따라서 설치를 해보길 바란다. 

<br>

[Airflow 설치 방법 링크](https://passwd.tistory.com/entry/Apache-Airflow-%EC%84%A4%EC%B9%98)

<br>

링크를 타기 귀찮은 분들을 위해 간단히 옮겨 적자면 아래 커맨드를 순서대로 터미널에 입력하면 된다. 

<br>

```bash
mkdir ~/airflow
export AIRFLOW_HOME=~/airflow
AIRFLOW_VERSION=2.3.2
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
```

<br>

<br>

# 3. Airflow 계정 생성 (옵션)

<br>

Airflow는 워크플로우를 확인할 수 있는 UI를 제공한다. 

여기에 접근하기 위해서는 ID와 비밀번호가 필요하다. 

처음에 공용으로 admin 계정을 제공하기는 하나 여러 명이서 사용할거라면 개인 계정은 하나씩 만드는게 좋다. 

커맨드에 아래의 명령어를 입력하면 쉽게 계정을 만들 수 있다. 

<br>

```bash
airflow users create -u <YOUR_ID> \
                     -f <YOUR_FIRSTNAME> \
                     -l <YOUR_LASTNAME> \
                     -e <YOUR_EMAIL> \
                     -r Admin \ 
                     -p <YOUR_PASSWORD>
```

<br>

- `-u`: 계정 아이디 
- `-f`: 사용자의 이름
- `-l`: 사용자의 성 
- `-e`: 사용자의 이메일 
- `-r`: 사용자의 역할(권한), Adim 이외에 Public, User(개발자), Viewer, Op등이 있음 
- `-p`: 사용할 비밀번호 

<br>

생성된 계정은 아래의 명령어를 통해 확인할 수 있다.

<br>

```bash
airflow users list
```

출처: [곰탱푸닷컴](https://www.bearpooh.com/150)


<br>

<br>

# 4. Python으로 워크플로우 파이프라인(DAG) 만들기 

<br>

Airflow는 워크플로우를 DAG(Directed Acylic Graph) 형태로 관리한다. 

아래와 같이 생긴 그래프인데 명확한 방향성이 있고 이전의 노드로 다시 돌아가는 루프가 없어 절차적인 업무를 표현할 때 좋다. 

<br>

![dag](/assets/backend_mlops/dag.png)

<br>

Airflow도 파이썬을 통해 이러한 DAG를 생성하며 이를 통해 전체 워크플로우를 관리한다. 

사람마다 대그(DAG), 워크플로우, 파이프라인 등등 다양하게 부르는데 나는 파이프라인으로 명칭을 통일하겠다. 

<br>

**airflow에서는 위와 같은 그래프를 대그(DAG)라고 부르고 각각의 노드(하나의 작업)을 task라고 부른다.** 

<br>

Airflow로 파이프라인을 만드는 과정은 쉽다. 

기존 전처리 코드에 몇줄만 추가해주면 된다. 

이를 위해 airflow에서 제공하는 라이브러리 몇개를 먼저 보자 

<br>

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
```

<br>

airflow는 위의 라이브러리 단 두개만을 이용해 기존의 파이썬 함수를 airflow의 task로 변경할 수 있다. 

예를 들어 아래와 같은 파이프라인을 구축하길 원하고 이를 우해 두 개의 파이썬 함수를 구현해뒀다고 가정해보자. 

<br>

![airflow_task](/assets/backend_mlops/airflow_example.png)

<br>

```python
# 원본 파이썬 함수 

def add(num1: int,
        num2: int):
    return num1 + num2 
    
def dot(num1: int,
        num2: int):
    return num1 * num2
```

<br>

먼저 **airflow 파이프라인을 정의해주기 위해서는 DAG 객체를 생성**해야한다. 

pytorch나 keras에서 Sequential 함수를 정의하고 하나씩 add하는 것과 비슷하게, 미리 생성해둔 DAG 객체에 task(파이썬 함수)를 하나씩 추가하는 형식이다. 

쉽게 설명하면 dag라는 리스트를 만들어두고 그 안에 task를 하나씩 append하는거라고 생각하면 된다.(당연히 정확한 설명은 아니다. 이해를 쉽게 하기 위한 예시이다.) 

<br>

DAG 객체는 아래와 같이 정의할 수 있다. 

<br>

```python
from airflow import DAG 

dag = DAG(dag_id=<YOUR_DAG_NAME>,
          start_date=<YOUR_START_DATE>,
          schedule_interval=<YOUR_SCHEDULE_INTERVAL)
```

<br>

- dag_id: airflow webUI에 출력되는 DAG 이름 
- start_date: DAG를 처음 시작하는 날짜(스케줄링 시 사용) 
- schedule_interval: DAG의 실행 주기(스케줄링 시 사용) 

<br>

이 외에도 변수의 기본 입력 값, 설명등 다양한 파라미터가 있다. 

이렇게 DAG 객체를 정의하고 나면 이제 **파이썬 함수를 task화 하고 이를 DAG의 컴포넌트로 할당**해주어야한다. 이 과정을 파이썬으로 보면 다음과 같다. 

<br>

```python
from airflow.operators.python import PythonOperator

# STEP1. DAG의 노드(Task) 정의 
add_task1 = PythonOperator(task_id="add",
                          python_callable=add,
                          op_args=[1, 2]
                          dag=dag)

add_task2 = PythonOperator(task_id="add",
                          python_callable=add,
                          op_args=[3, 4]
                          dag=dag)
                          
dot_task = PythonOperator(task_id="dot",
                          python_callable=dot,
                          provide_context=True,
                          dag=dag)
                          
   
# STEP2. DAG의 엣지(의존성) 정의 
add_task1 >> dot_task
add_task2 >> dot_task
```

<br>

STEP1과 같이, airflow에서 제공하는 PythonOperator 함수를 통해 기존의 파이썬 함수를 간단하게 Task로 변환할 수 있다. 

`python_callable` 변수에 원하는 함수의 이름을 적어두면 된다. 

그리고 입력 변수가 있는 경우, `op_args`나 `op_kwargs` 변수에 변수를 지정해주면 된다. 

마지막으로, dag 변수에 기정의해둔 dag 변수를 적어두면 파이프라인에 원하는 task를 할당할 수 있다. 

<br>

그런데 여기서 dot function을 자세히 보도록 하자. 

dot 함수는 이전 add task에서 생성된 output을 받아 곱하는 함수이다. 

그러면 입력 변수로 `add_task1`의 결과값과 `add_task2`의 결과 값을 받아야한다. 

애석하게도 airflow에서는 이 함수의 결과값을 받는 부분이 다소 불친절하게 구현되어 있다. 

<br>

**앞서 설명했듯 airflow는 이러한 함수 간의 변수 이동이 다소 제한적**이다. 

위의 코드를 보면, dot 함수는 add 함수의 결과를 받아야한다. airflow에서는 이전 함수에서 생성된 결과를 context에 저장해두고 있으며, xcom_pull이라는 함수를 통해 이를 불러올 수 있다.(참고로 xcom은 cross communication의 약어이다) 

`xcom_pull` 함수에서 원하는 task의 id를 파라미터로 주면 해당 task의 아웃풋을 불러올 수 있다. 

그리고 이전 add1 task에서 계산된 결과 값과 add2 task에서 연산된 결과를 각각 변수에 저장 후 이를 곱하는 연산을 수행한다. 

때문에 dot 함수는 아래와 같이 변경되어야한다.  

<br>

```python
def dot(**context):
    add1_result = context["task_instance"].xcom_pull(task_ids="add1")
    add2_result = context["task_instance"].xcom_pull(task_ids="add2")
    return add1_result * add2_result
```

<br>

이렇듯 **함수 간 변수 이동이 다소 복잡하기 때문에, 내 경우에는 함수 마지막에 결과물을 저장한 뒤에 다음 함수에서 다시 read해오는 식으로 함수를 구현**한다. 

STEP2는 만들어진 task를 모아 하나의 파이프라인으로 구축해주는 과정이다. **task간의 연결 관계를 구축하는 과정이라고 보면 된다. Airflow에서는 \>> 을 통해 이 관계를 간단히 정의할 수 있다.** 

파이썬을 통해 Airflow 파이프라인을 구축하는 과정을 총 정리하면 다음과 같다. 

<br>

```python
from airflow import DAG 
from airflow.operators.python import PythonOperator

# STEP1. 미리 구현해둔 파이썬 함수 
def add(num1: int,
        num2: int):
    return num1 + num2 
    
# dot 함수는 add 함수의 결과를 받아야하므로 함수가 살짝 바뀜 
# 보통 이렇게 구현하는 경우는 잘 없으니, 이부분은 이해가 안되면 넘어가시고 STEP2부터 주의깊게 보시면 됩니다.
def dot(**context):
    add1_result = context["task_instance"].xcom_pull(task_ids="add1")
    add2_result = context["task_instance"].xcom_pull(task_ids="add2")
    return add1_result * add2_result
    
# STEP2. DAG 객체 정의 
dag = DAG(dag_id=<YOUR_DAG_NAME>,
          start_date=<YOUR_START_DATE>,
          schedule_interval=<YOUR_SCHEDULE_INTERVAL>)
    
# STEP3. DAG의 노드(Task) 정의 
add_task1 = PythonOperator(task_id="add",
                          python_callable=add,
                          op_args=[1, 2]
                          dag=dag)

add_task2 = PythonOperator(task_id="add",
                          python_callable=add,
                          op_args=[3, 4]
                          dag=dag)
                          
dot_task = PythonOperator(task_id="dot",
                          python_callable=dot,
                          provide_context=True,
                          dag=dag)
                          
   
# STEP4. DAG의 엣지(의존성) 정의 
add_task1 >> dot_task
add_task2 >> dot_task
```

<br>

이렇게 하면 간단하게 파이썬 코드를 통해 Airflow 파이프라인을 구축할 수 있다. 

Airflow는 파이썬 이외에도 Shell script, E-mail 발송용 operator등 다양한 오퍼레이터를 제공한다. 

Airflow를 더 딥하게 사용해보고 싶은 사람들은 직접 해당 오퍼레이터를 찾아보며 적용해보길 바란다. 

<br>

마지막으로 내가 만들어둔 Airflow 작성 템플릿을 공유하도록 하겠다. 

템플릿에서는 입력 변수를 Kwargs 형태로 정의했다. 

회사 내부 공유용으로 만든 템플릿이라 다른 사람들에게는 얼마나 유용할지 모르겠지만, 다만 한명이라도 이 템플릿을 통해 Airflow 시스템을 잘 구현했다고 하면은 매우 뿌듯할 것 같다. 

<br>

```python
from airflow import DAG
from airflow.operators.python import PythonOperator

"""
Your Python Code in Here 

각 파이프라인의 노드를 task라고 부릅니다.
각 task는 하나의 Function에 대응됩니다. 

평소에 코딩하시듯 함수 단위로 전처리 코드를 구현하시면 됩니다. 
여기서 구현되는 함수들이 하나의 task가 됩니다. 
"""

# ============================================================
# XXX STEP 1. DAG(Directed Acyclic Graph) 인스턴스 정의. 
# Pytorch의 Sequential처럼 DAG 인스턴스를 먼저 선언해두고 Task를 하나씩 더하는 방식입니다. 
dag = DAG(dag_id="<YOUR_DAG_NAME>",                                     # Airflow UI에 표시되는 DAG 명 
          start_date="<DATE_THAT_YOUR_PIPELINE_START>",                 # 파이프라인이 처음 시작되는 날짜(스케줄링 해두면 해당 날짜를 기준으로 파이프라인이 자동으로 실행됨)
          schedule_interval="<SCHEDUL_INTERVAL_OF_YOUR_PIPELINE>",      # 파이프라인 실행 주기 
          description="<YOUR_PIPELINE_DESCRIPTION>")                    # 파이프라인에 대한 설명 

# XXX STEP 2. TASK 정의.
# 위에서 구현한 파이썬 함수들을 Task로 변환해줍니다. 
# 함수에 입력되는 파라미터가 있다면 미리 딕셔너리 형태로 정의해두면 됩니다. 
# 본 template에서는 pythonoperator만 넣어두었음 
# Bash operator는 아래의 블로그를 참고하길 바람 
# https://pbj0812.tistory.com/390
task1_params = {
    "param1": "param1",
    "param2": "param2"
}

task2_params = {
    "param3": "param3",
    "param4": "param4"
}

task1 = PythonOperator(task_id="<YOUR_TASK_NAME>",                      # Task 명
                       python_callable="<YOUR_PYTHON_FUNCTION_NAME",    # 파이썬 함수명(변수명 그대로 쓰면됨) 
                       op_kwargs=task1_params,                          # 파이썬 함수의 입력 파라미터(Airflow에서 직접 기입할수도 있음)
                       dag=dag)                                         # Task가 실행될 DAG 인스턴스 지정 
task2 = PythonOperator(task_id="<YOUR_TASK_NAME>",
                       python_callable="<YOUR_PYTHON_FUNCTION_NAME",
                       op_kwargs=task2_params,
                       dag=dag)


# XXX STEP 3. Task를 연결해줍니다. 
# Airflow는 >> 로 간단하게 파이프라인을 연결해줄 수 있습니다. 
# 예를 들어, task1 뒤에 task2가 수행된다고 가정해보겠습니다. 
task1 >> task2
```

<br>

<br>

# 5. 끝으로

<br>

원래 Airflow 설치, 파이프라인 구축, 파이프라인 업로드, webserver 접속 및 파이프라인 실행까지 이 포스팅에서 다 다루려고 했는데, 내용이 너무 길어져버렸다. 

때문에 파이프라인 업로드와 webserver 실행은 다음 포스팅에서 다루도록 하겠다. 

<br>

**요약**

<br>

- Airflow는 파이썬 기반의 데이터 처리 워크플로우를 작성, 스케줄링 할 수 있는 강력한 도구이다. 
- Airflow는 DAG로 데이터 파이프라인을 구성한다. 
- 기존의 파이썬 함수에 airflow의 PythonOperator를 wrapping하여 파이프라인의 노드를 구성할 수 있다. 
- \>> 를 통해 DAG 파이프라인을 생성할 수 있다.

<br>

<br>
---
layout: article
title: FastAPI로 모델 서빙하기
aside:
  toc: true
cover: /assets/backend_mlops/fastapi_logo.png
excerpt: FastAPI를 이용해 머신러닝 모델을 서빙하는 방법에 대해 알아보겠습니다.
---

최근 사이드 프로젝트로 챗봇을 만들고 있습니다.  
   
AI 모델링이야 지난 몇년동안 매일 해오던 일이니 금방 구축을 했죠.  
   
그런데 생각해보니 제가 그동안 직접 배포를 해본적이 없더라고요.  
  
거기서 막혀서 좀 해매긴 했지만 그래도 어찌저찌 해내긴 했습니다.  
   
그래서 오늘은 제가 머신러닝 모델을 배포하며 공부했던 내용을 공유해보겠습니다.

혹시 API가 뭔지 모르시는 분들은 아래 포스팅을 참고해주세요. 

 [API가 뭐지?](https://kiminjo.github.io/mlops/2024-05-26-fastapi/)

<br>

<br>

# 1. 배포 도구: FastAPI

<br>

![fastapi_logo](/assets/backend_mlops/fastapi_logo.png)

<br>
   
모델 배포 툴로는 `FastAPI`를 사용했습니다.  
   
FastAPI를 사용한 이유는 단순한데요.

<br>

> 1. Flask, django 대비 통신 속도가 빠르다.  
> 2. 사용이 쉽다.  
> 3. 공식 문서가 매우매우 친절하다.

<br>

특히, 공식 문서가 친절한게 너무너무 좋더라고요.  
   
처음 사용하는 툴은 그 툴만의 용어나 문법에 적용하기 어려워서 여러 블로그 글을 읽어봐야하잖아요.  
   
그런데, FastAPI는 공식 문서가 너무너무 친절하게 작성되어 있어고 예제도 많아서 그것들만 그대로 따라가도 저만의 웹서버를 뚝딱 구축할 수 있었습니다.  
 
<br>

<br>

# 2. LLM 모델 추론기 구축하기 

<br>

다들 아시겠지만, RestAPI는 HTTP 메서드를 사용하죠.  
   
모르셔도 상관은 없어요.  
   
그냥 컴퓨터 2대가 서로 대화하는데 동사가 딱 4개 밖에 없다고 생각하시면 돼요.  

<br>
 

4개의 동사는 `Get(데이터 받기)`, `Put(데이터 보내기)`, `Post(데이터 저장)`, `Delete(데이터 삭제)`에요.

<br>

<br>

이 중에서 제가 LLM 모델을 구축하며 주로 사용했던 메서드는 `Get`과 `Post`였어요.

<br>
   
아무래도, 배포라는 작업 특성상, 데이터 저장이나 삭제를 할 일은 거의 없더라고요.  
   
그래서 두 메서드만으로 LLM 모델을 배포하는데 큰 어려움은 없었습니다.   
   
<br>
   
저는 웹서버를 띄워놓고 바로 추론기로 사용을 했어요.   
   
코드는 아래와 같이 구현했습니다.   

<br>

 
```python
# File name: server.py 

# Custom Module
from src import PersonaChatbot

# API
from fastapi import FastAPI
from pydantic import BaseModel

# Create the FastAPI app
app = FastAPI()

class ChatComponent(BaseModel):
    prompt: str
    api_key: str
    db_path: str

@app.get("/")
def root():
    return {"Hello": "World"}

@app.post("/chat/")
def chat_with_bot(chat_component: ChatComponent) -> dict:
    model = PersonaChatbot(api_key=chat_component.api_key,
                           db_path=chat_component.db_path)
    result = model.chat(chat_component.prompt)
    return {"output": result}
```

<br>

최대한 불필요한 요소는 덜어내고 코드를 작성해봤어요.   
   
root 함수는 그냥 메인 페이지 확인용으로 작성한거니 안보셔도 돼요.   
   
중요한 함수는 그 아래 chat_with_bot 함수입니다.   
   
해당 함수는 api_key와 vector db의 데이터 경로, 그리고 입력 프롬프트를 받아서, AI response를 반환하는 함수인데요.   
   
<br>
   
함수 바로 위에 보시면 `@app.post`라는 데코레이터가 위치해 있습니다.   
   
이 데코레이터는 FastAPI에서 제공하는 문법인데요.   
   
앞서 말씀 드렸듯, post는 데이터 전송 메서드라고 말씀 드렸죠.   
   
저 **데코레이터의 의미는 http://localhost:8000/chat/ 으로 정보를 전송하겠다**. 라는 뜻이에요.   
   
대체 무슨 정보를 전송하는 것이고, 또 전송하면 어떻게 되는걸까요?   
   
<br>
   
그 내용은 바로 아랫줄에 나와있는데요.   
   
**chat_component라는 변수 집합을 전송하고 그 결과로 `chat_with_bot`이라는 함수를 실행**한다는 뜻입니다.   
   
그러면 chat_with_bot 함수에서 LLM 모델 실행 후 AI response를 반환하는 구조인거죠.   
   
<br>
   
그리고, 그 chat_component의 구조는 root 함수 위에 정의해뒀어요.   
 
<br>

```python
class ChatComponent(BaseModel):
    prompt: str
    api_key: str
    db_path: str
```

<br>

이 함수의 의미는 누군가 `chat_with_bot` 함수를 호출할때, prompt, api_key, db_path를 명시해달라고 요청하는 것입니다.   
   
FastAPI는 Pydantic이라는 라이브러리를 이용해서 입출력 시 데이터 구조를 엄밀하게 제한하고 있어요.   
   
이에 대해서는 이후에 추가 포스팅을 다뤄볼게요.   
   
<br>
   
이렇게 파이썬으로 FastAPI 서버 코드를 구축한 후, uvicorn을 실행하면 웹서버가 실행돼요.   

<br>

```bash
uvicorn server:app --reload
```

<br>

여기서 server는 위의 서버 코드를 구축한 파이썬 파일명이고, app은 FastAPI 인스턴스를 생성한 변수명이에요.   
   
즉, server 파일 안의 app 변수를 따라 uvicorn으로 서버를 구축한다는 의미죠.   
   
`--reload`는 파이썬 코드 수정 시 이를 즉각적으로 서버에 반영하겠단 뜻입니다.   
   
지금 별도의 포트를 지정 안해줬는데, FastAPI는 기본적으로 8000번 포트를 사용해서 통신을 합니다. 

<br>

<br>

# 3. LLM 모델 추론하기(post로 모델 호출) 

<br>

그럼 저렇게 배포한 모델을 사용해서 추론을 진행해볼까요?   

<br>
 

```python
# File name: server_test.py

import requests
import os 

api_key = os.environ.get("OPENAI_API_KEY")
db_path = "vector_store/"
url = "http://127.0.0.1:8000/chat"

output = requests.post(url, json={"api_key": api_key, 
                                  "db_path": db_path,
                                  "prompt": "유퀴즈 출연 후 아쉬웠던 점?"})

print(output.json()["output"])
```

<br>

위의 코드는 requests 라이브러리를 이용해서 웹서버로 post 요청을 보내는 함수에요.   
   
url은 웹서버가 구축된 주소로 **FastAPI에서는 별다른 지정을 안해주면 localhosts(127.0.0.1)의 8000번 포트에 서버를 구축**합니다.   
   
<br>
   
앞선 코드에서 chat_with_bot 함수를 호출 시, api_key, db_path, prompt를 명시할 것으로 요청했어요.   
   
그래서 requests.post를 보면, 해당 3개 파라미터를 명시해주고 있는 걸 볼 수 있어요.   
   
<br>
   
보시는 것처럼 FastAPI는 기본적으로 RestAPI를 지원하다보니 json 형태로 데이터를 주고 받습니다.   
   
가운데에 requests.post를 보낼때 입력 데이터도 json 형태로 보내고 있죠.   
   
그리고 마지막 줄에 출력을 받을때도, json 형태로 반환해서 사용합니다.   
   
<br>

<br>

# 4. 마치며

<br>

이렇게 간단하게 웹서버를 구축해 보았고 또 post 메서드를 통해 모델 추론을 진행해봤습니다.   
   
처음 웹서버를 구축해보는거지만, FastAPI가 원채 사용이 쉽고 문서도 친절하다보니 별 어려움 없이 성공할 수 있었네요.   
   
<br>
   
제 경우에는 제로샷 LLM 모델을 사용하다보니 별도의 모델 파라미터가 필요없어 추론기 내부에 LLM 모델을 선언해줬어요.   
   
하지만 딥러닝 시스템을 구축하다보면 학습된 pickle 파일이나 pth 파일을 불러와야하는 경우도 있습니다.   
   
이 경우, 별도의 모델 호출 API를 정의해주는게 좋아요.   
   
해당 내용도 언젠가 기회가 된다면 포스팅해보도록 하겠습니다.   
   
<br>
   
제 글이 FastAPi를 처음 접하시는 분께 작은 도움이 됐으면 좋겠습니다 :)

<br>

<br>